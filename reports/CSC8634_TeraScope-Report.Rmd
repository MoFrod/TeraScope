---
title: "CSC8634_TeraScope Report"
author: "Morgan Frodsham (210431461)"
date: "10/01/2022"
output: pdf_document
citation_package: natbib
bibliography: "references.bib"
biblio-style: "apalike"
link-citations: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r ProjectTemplate, include = FALSE}
library(ProjectTemplate)
load.project()
```

# Extended Technical Project: Performance Evaluation of Terapixel Rendering in Cloud (Super)computing

## Understanding

### Background 

This report details the process and findings of an extended technical project on Newcastle University's cloud supercomputer architecture for visualising environmental data captured by the Newcastle Urban Observatory. It explores the data created during the computational production of [this] (http://terapixel.wasabi.1024.s3.eu-central-1.wasabisys.com/vtour/index.html) visualisation of Newcastle upon Tyne. Newcastle University and the Newcastle Urban Observatory started this initiative with the idea that "the only way to understand how to monitor a city is to try and monitor one" with a monitoring platform that matches "the scale, complexity and scope of our modern cities" (@UO). It offers an unparalleled opportunity to improve decision making the city by accurately measuring and analysing our urban environment. 

### Objectives and success criteria 

Urban Observatories attempt to replicate the "breadth, longevity and success of astronomical observatories"(@UO) in understanding how cities operate. Funded in partnership with the UK Collabatorium for Research in Infrastructure and Cities (@UKCRIC), the Newcastle Urban Observatory collects environmental data about the city of Newcastle-upon-Tyne (@NUO). This is considered to be the most expansive set of "environmental data in the UK, with: over 74 urban indicators; 7,000 observations every minute; 3,600 sensor streams; 9 billion data points; 540 CCTV sensors; and hundreds of millions of images" (@Parliament). Newcastle University has created a scalable cloud supercomputer software architecture for visualising this data as realistic terapixels (@Terascope). Terapixels are images containing over one trillion pixels (@Terapixel) that allow viewers to interactively browse big data in intense detail across multiple scales (@Terascope). The terapixel visualisation created by Newcastle University's cloud supercomputer allows the viewer to "zoom in from an overview of just one square kilometre of the city... to see detail within a single room in an office or a house" (@Terapixel).

In delivering this initative, Newcastle University had three objectives:

1. "create a supercomputer architecture for scalable viualisation using the public cloud;

2. produce a terapixel 3D city viusalisation supporting daily updates; and 

3. undertake a rigorous evaluation of cloud supercomputing for compute intensive visualisation applications" (@Terascope).

This extended technical project focuses on the third objective. It explores three datasets created from the data produced by the supercomputer architecture during the creation of the terapixel visualisation, and aims to provide useful insights into the computational performance. The process and findings of this report explore the question: "How could the computation of a terapixel visualisation be more efficient?". The success criteria of the project is to identify useful insights about the stages of terapixel computation which are most resource intensive. Developing a better understanding of this is important as producing a high quality terapixel image is an intense process with significant computational costs (resource, energy, environmental and monetary) (@Terapixel). 

With a growing focus on doing "more with less" (@Google), cloud computing suppliers and users are trying to continue scaling with fewer computational costs (such as fewer servers and less energy). For humankind to fully reap the benefits of information provided by these visualisations, the computation needs to be efficient and associated costs must be sustainable. Improving the efficiency cloud computing has been a core focus for suppliers and users; indeed, Google returns 90.9 million results for "cloud computing efficiency". As the likelihood and potential impact of climate change is fully realised, users as well as wider society are increasingly concerned about the sustainability of digital technologies, such as cloud computing. Just five organisations - Amazon, Google, Microsoft, Facebook and Apple - use as much electricity annually as New Zealand (@FT). Moreover, the Shift Project suggests that tech-related emissions are rising by 6 per cent annually (@FT). Data centres are estimated to account for "1% of the total global energy demand" (@Environmental) Investigating the resource intensity of computing intensive visualisation applications, such as terapixels, is therefore a vital component of a rigorous evaluation and a necessary step towards greater sustainability. 

### Data mining goals and success criteria

To explore the question "How could the computation of a terapixel visualisation be more efficient?" this extended technical project seeks to identify useful insights about the stages of terapixel computation which are most resource intensive. To achieve this, the goals of data mining are:

1. Which events dominate task duration (run time)? The data mining success criteria is calculating the duration of each event to determine which has the longest duration. 

2. What is the relationship between duration and GPU power draw? The data mining success criteria is identifying the power needed for duration time to better understand what demands more GPU power.  

3. What is the relationship between GPU power draw and performance? The data mining success criteria is identifying the impact of GPU temperature, core utilisation and memory demand on GPU power to understand what demands more power.


- we should see efficientl linear scalaing as we add more compute notdes - gustafosn-Baris' law @Terapixel Rendering images is considered to be a good test of hardware performance as it is capable of absorbing all available compute resource, following Gustafson-Barsis' law that the problem size scales to fill the compute capacity (@Terapixel). 

## Tools


This project utilises the following terminology, tools and techniques: 

- R, which is a programming language for statistical computing and graphics.

- RStudio, which is an Interated Development Environment for R.

- R packages, which are extensions to the R statistcial programming language. R packages contain code,
data, and documentation in a standardised collection format that can be installed by users of R. The
R packages used by this report are listed in the section below.

- Git, which is software for tracking changes in any set of files, usually used for coordinating work among
programmers collaboratively developing source code. GitHub is a provider of internet hosting for
software development and version control using Git. Git Bash is an application for Microsoft Windows
environments which provides an emulation layer for a Git command line experience.

- Tibble, which is a data frame that stores data in R and appears as a table.

- CRISP-DM (Chapman et. al. and Wirth (2000)) is the methodology used to deliver data workflow
best practice;

- Tidyverse for R is the R package used for the analysis;

- ggplot2 for R is the R package used for visualisations;

- RColorBrewer is the R package used to provide accessible colour palettes for visualisations;

- ProjectTemplate is the R package used to improve the reproducibility of this analysis;

- [SHINY?]

- all written documentation is created with the R package, RMarkdown; and

- natbib is an R package used for the bibliography


## Data Understanding

### Initial data collection

The data for this extended technical project was created during a run (from application checkpoint and system metric output) using 1024 GPU nodes, and this run processes three levels of visualisation (levels 4, 8 and 12) to render the terapixel (@Terascope). It is unclear whether there have been any problems in data acquisition or extraction as Newcastle University has provided the data.

The data provided shows the timings of producing the terapixel, GPU card performance, and the coordinates for the part of the terapixel image that was rendered in each task (@Terascope) in three different data sets. This data appears to be raw, but of sufficient quality to be tided and analysed. It will provide sufficient information for an initial investigation into the stages of terapixel computation process which are most resource intensive.

### Data description

The run to compute the terapixel visualisation provides the following data sets:

- the application checkpoint events throughout the execution of the render job (timestamp, hostname, eventName, eventType, jobId, taskId);

- the metrics output relating to the status of the GPU on the virtual machines (timestamp, hostname, gpuSerial, gpuUUID, powerDrawWatt, gpuTempC, gpuUtilPerc, gpuMemUtilPerc); and 

- the x,y co-ordinates of the part of the image being rendered for each task (jobId, taskId, x, y, level).

### Data exploration 

The data exploration outlined in this report began with initial questions to better understand what each data set contained and visual investigation to obverse any obvious patterns. Further details of this initial exploration can be seen in the file 'eda.R' in the src folder. Examples are shared below:

```{R hosts, include=FALSE}
# How many unique hostnames are there?
hosts <- AC1 %>%
  group_by(hostname) %>%
  summarise (n_distinct(hostname)) %>%
  count() 
```

1. There are 1024 unique host names, which suggests that these are the names of each virtual machine containing the GPU nodes. 

```{R serials, include=FALSE}
# How many GPU serials are there?
gserials <- GPU1 %>%
  group_by(gpuSerial) %>%
  summarise(n_distinct(gpuSerial)) %>%
  count() 
```

2. There are 1024 unique GPU serials, which suggests that each virtual machine has one GPU node with its own unique serial number. 

```{R totalrender, include=FALSE}
# How many times does TotalRender happen per host?
TRh <- AC1 %>%
  filter(eventName == "TotalRender") %>%
  count() %>%
  as_tibble() %>%
  arrange(desc(n), .by_group = FALSE)
```

3. It appears that some virtual machines complete more 'TotalRender' than others, suggesting that some are more productive.

```{R taskId, include=FALSE}
# How many taskIds are there?
ID <- AC1 %>%
  group_by(taskId) %>%
  summarise(n_distinct(taskId)) %>% 
  count() %>% 
  print() # This is the same in both application.checkpoints and task.x.y
```

4. There are 65793 taskIds in the applications.checkpoint data set (copied as AC1), which matches the number of taskIds in the task.x.y data set.

```{R jobID, include=FALSE}
# How many jobIds are there?
j <- AC1 %>%
  group_by(jobId) %>%
  summarise(n_distinct(jobId)) %>%
  count() %>%
  print() # This is the same in both application.checkpoints and task.x.y, and corresponds to the levels 4, 8 and 12
```

5. There are 3 distinct jobIds in the applications.checkpoint data set (copied as AC1), which is the same in task.x.y, and seems to reflect the three visualisation levels (level 4, 8 and 12). 

To address the key objective and success criteria, the exploratory analysis outlined in the rest of this section starts with applications.checkpoints, moves to gpu, and finishes with the task.x.y in line with the data mining goals and is structured by those goals. Each goal has it's own eda file in the src folder.

**Goal 1 (Run time)**

To create a terapixel, Newcastle University render levels 4, 8 and 12, "and subsample these to fill in each set of three intermediate levels" (@Terapixel). There are 5 different events (eventName) that happen during the creation of a visualisation: Saving Config, Render, Tiling, Uploading and TotalRender. The data set application.checkpoints records the timestamp of when these events start and stop (eventType). 

To determine which events have the longest duration (which means that they dominate the run time), the application.checkpoints data set is copied as AC1. AC2 applies a pivot wider function to AC1 so that eventType 'START' and 'STOP' become columns for the corresponding timestamp for each eventName. Both columns are mutated so that their values are date and time. In a new column, duration is calculated by subtracting START from STOP.

```{R AC2, include=FALSE}
# Display AC2 with START, STOP and duration as columns
print(AC2, n = 6, width = Inf)
```

To investigate the duration of each event, a boxplot was created of duration by event name.

```{R duration, echo= FALSE}
# Plot duration of eventName
ggplot(AC2, aes(x = eventName, y = duration)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Name of Event", y = "Duration (seconds)", title = "Boxplot of Duration by Name of Event") + theme(legend.position = "none")
```

This highlights that 'TotalRender' has the longest duration. It is assumed that this is because it includes the duration of all the other events.

AC3 is created by filtering out TotalRender from AC2. Another boxplot was created of duration by event name.

```{R duration1, echo = FALSE}
# Remove Total Render from results
AC3 <- AC2 %>%
  filter(eventName != "TotalRender")

# Plot duration of eventName activities without total render
ggplot(AC3, aes(x = eventName, y = duration)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Name of Event", y = "Duration (seconds)", title = "Boxplot of Duration by Name of Event without TotalRender") + theme(legend.position = "none") 
```

It is clear that 'Render' has the longest duration, and therefore is the event that dominates the run time. 

```{R longestrender, echo = FALSE}
# Which hosts have the longest Render time?
AC6 <- AC3 %>%
  filter(eventName == "Render") %>%
  arrange(desc(duration)) # Arrange so longest duration is at the top

R_long <- head(AC6, 50) %>%
  group_by(hostname) %>% # Isolate the 50 longest durations
  print(., n = 5, width = Inf)
```

The hostnames of the virtual machines with the longest Render durations (over 74 seconds) are:

1. 0d56a730076643d585f77e00d2d8521a00000I with 81.51 seconds;

2. 4a79b6d2616049edbf06c6aa58ab426a000003 with 80.16 seconds;

3. 95b4ae6d890e4c46986d91d7ac4bf08200000G with 77.59 seconds;

4. 6139a35676de44d6b61ec247f0ed865700000I with 75.12 seconds; and 

5. 5903af3699134795af7eafc605ae5fc700000U with 74.02 seconds. 

Further exploration related to this data goal can be viewed in EDA1.

**Goal 2 (Power draw)**

To identify the power needed for duration time, the application.checkpoints and gpu data set needed to be combined. This could be achieved by either timestamp or hostname as the common values across both data sets. Joining by hostname was unlikely to provide us with the desired result as hostnames appeared more than once in both data sets.

The code chunk below sets out how the two data sets were joined by timestamp.

```{R Power_Duration2}
# Create a single data set with power usage and duration for every event other than total render
BEGIN <- as_datetime("2018-11-0807:41:27.242", tz = "Europe/London")
END <- as_datetime("2018-11-0807:42:27.242", tz = "Europe/London")

AC9 <- AC2 %>%
  filter(START >= BEGIN, START <= END) %>% # Select start times between BEGIN and END
  mutate(timestamp = round_date(START, unit = "2 seconds")) # Create new timestamp column where start time is rounded to the nearest 15 seconds

GPU2 <- GPU1 %>%
  filter(timestamp >= BEGIN, timestamp <= END) %>% # Select timestamp that is between BEGIN and END
  mutate(timestamp = round_date(timestamp, unit = "2 seconds")) # Alter the timestamp column so that it is rounded to the nearest 15 seconds

Power_Duration2 <- full_join(AC9, GPU2, by = c("timestamp", "hostname")) %>% # Join AC8 and GPU2
  na.omit() %>% # Omit rows with NA
  group_by(hostname) %>% # Group by hostname
  arrange(timestamp, .by_group = TRUE) # Arrange by timestamp
```

Initially, a boxplot was created of power draw by event name to see if there was an obvious correlation between duration and power draw.

```{R EventPower, echo = FALSE}
# Plot event name by power draw
ggplot(Power_Duration2, aes(x = eventName, y = powerDrawWatt)) + geom_boxplot(alpha = 0.3, color = "#33a02c") + labs(x = "Name of Event", y = "Power Draw (watts)", title = "Boxplot of Power Draw by Name of Event without TotalRender") + theme(legend.position = "none")
```

Interestingly, the events with the highest duration (TotalRender and Render) do not have the largest power draw. Tiling and Upload seem to consume the most power.

The combined data set was then filtered to see the power draw by duration for 'Tiling'. This was graphed in a scatter plot, coloured by 'jobId', below:

```{R TPowerDuration, echo = FALSE}
# SAVE AND LABEL # Plot hostname by power draw and duration of tiling
Power_Duration2 %>%
  filter(eventName == "Tiling") %>%
  ggplot(aes(x = duration, y = powerDrawWatt, color = jobId)) + geom_point(position = "jitter") + labs(x = "Duration (seconds)", y = "Power Draw (watts)", title = "Tiling: Power Draw by Duration") + theme(legend.position = "bottom") + scale_color_brewer(palette = "Paired")
```
Given that we know there are 2 jobIds, it is interesting to observe that most of the tiling occurs for the jobId of level 12 (1024-lvl12-7e026be3-5fd0-48ee-b7d1-abd61f747705). It is unclear if this is a problem with the data wrangling, or a true result. Further investigation is required. 

The combined data set was then filtered to see the power draw by duration for 'Uploading'. This was graphed in a scatter plot, coloured by 'jobId', below:

```{R UPowerDuration, echo = FALSE}
# Plot hostname by power draw and duration of uploading
Power_Duration2 %>%
  filter(eventName == "Uploading") %>%
  ggplot(aes(x = duration, y = powerDrawWatt, color = jobId)) + geom_point(position = "jitter") + labs(x = "Duration (seconds)", y = "Power Draw (watts)", title = "Uploading: Power Draw by Duration") + theme(legend.position = "bottom") + scale_color_brewer(palette = "Paired")
```

The same pattern is observed in uploading as for tiling, where there is only power draw and duration data for 2 jobIds (level 8 and 12). Interestingly, here there are also two distinct groups for duration. This would benefit from further exploration.

The combined data set was then filtered to see the power draw by duration for 'TotalRender'. This was graphed in a scatter plot, coloured by 'jobId', below:

```{R TPowerDuration1, echo = FALSE}
# SAVE AND LABEL # Plot hostname by power draw and duration of total render
Power_Duration2 %>%
  filter(eventName == "TotalRender") %>%
  ggplot(aes(x = duration, y = powerDrawWatt, color = jobId)) + geom_point(position = "jitter") + labs(x = "Duration (seconds)", y = "Power Draw (watts)", title = "TotalRender: Power Draw by Duration") + theme(legend.position = "bottom") + scale_color_brewer(palette = "Paired")
```

There are 3 jobIds visible in the scatter plot for TotalRender, although data for level 4 (1024-lvl4-90b0c947-dcfc-4eea-a1ee-efe843b698df) only appears once. It also seems that there are 3 groups for duration.

In the scatter plots above, however, there is not an obvious correlation between power draw and duration. In the tiling and uploading scatter plots it seems that there is some sort of OFF/ON division in the data, as there is an observable gap in the power draw. There also seems to be some sort of limitation on the virtual machine, perhaps specific virtual machines are tasked with tasks that are more intensive as there appears to be three separate groups by duration,  not necessarily by jobId (visualisation level). 

To better understand whether there is any observable pattern, the mean, standard deviation and coefficient of variation for power draw and duration was calculated in the code chunk below. 

```{R HostAverage, echo = FALSE}
# What is the mean, standard deviation and coefficient of variation for Total Render duration?
AC7 <- AC2 %>%
  filter(eventName == "TotalRender") # Filter to create a new data set only with TotalRender

TR1 <- AC7 %>%
  group_by(hostname) %>% # Group by the virtual machine host name
  summarise(av_duration = mean(duration), sd_duration = sd(duration)) %>% # Calculate mean and standard deviation of duration for each host.
  mutate(CoV = (sd_duration/av_duration)*100) # New column with coefficient of variation

# What is the mean, standard deviation and coefficient of variation for GPU power draw?
Power <- GPU1 %>%
  group_by(hostname) %>% # Group by hostname
  summarise(av_power = mean(powerDrawWatt), sd_power = sd(powerDrawWatt)) %>% # Calculate mean and standard deviation of power for each host.
  mutate(CoV_p = (sd_power/av_power)*100) # New column with coefficient of variation

# Create single data set to compare execution time and power draw. 
Power_Duration <- left_join(TR1, Power, by = "hostname")
```

These values are combined into a single data set and visualised with the scatter plot below.

```{R HostPowerDuration, echo = FALSE}
# Plot average power draw by average duration for each hostname
Power_Duration %>%
  ggplot(aes(x = av_duration, y = av_power)) + geom_point(position = "jitter", color = "#33a02c") + geom_smooth(color = "#525252") + labs(x = "Average Duration (seconds)", y = "Average Power Draw (watts)", title = "Average Power Draw by Average Duration for Hostname") 
```

It does appear that there are two clear groups of virtual machines (hostname), but it is unclear why. 

**Goal 3 (Performance)**

To understand the relationship between GPU power draw and performance, the impact of GPU temperature, core utilisation and memory demand on GPU power needs to be determined. 

First, the mean, standard deviation and coefficient of variation are calculated.

```{R P_stat, }
# What is the mean, standard deviation and coefficient of variation for all GPU metrics by host
Performance <- GPU1 %>%
  group_by(hostname) %>% # Group by hostname
  summarise(av_power = mean(powerDrawWatt), sd_power = sd(powerDrawWatt), av_temp = mean(gpuTempC), sd_temp = sd(gpuTempC), av_util = mean(gpuUtilPerc), sd_util = sd(gpuUtilPerc), av_mem = mean(gpuMemUtilPerc), sd_mem = sd(gpuMemUtilPerc)) %>% # Calculate mean and standard deviation of power for each host.
  mutate(CoV_p = (sd_power/av_power)*100, CoV_t = (sd_temp/av_temp)*100, CoV_u = (sd_util/av_util)*100, CoV_m = (sd_mem/av_mem)*100) # New column with coefficient of variation
```


Can I turn the co-effieinct of variation into a percentage?
```{R temp.power, echo = FALSE}
# Plot power by temperature
Performance %>%
  ggplot(aes(x = av_temp, y = av_power, color = CoV_t)) + geom_point(position = "jitter") + geom_smooth(color = "#525252") + labs(x = "Average Temperature (celcius)", y = "Average Power Draw (watts)", title = "Average Power Draw by Average Temperature for Hostname")  # Temperature is loosely correlated to power; temperature seems to increase as you use more power
```

```{R util.power, echo = FALSE}
# Plot power by util
Performance %>%
  ggplot(aes(x = av_util, y = av_power, color = CoV_u)) + geom_point(position = "jitter") + geom_smooth(color = "#525252") + labs(x = "Average Utilisation (percentage)", y = "Average Power Draw (watts)", title = "Average Power Draw by Average Core Utilisation for Hostname")
```

```{R mem.temp, echo = FALSE}
# Plot power by mem
Performance %>%
  ggplot(aes(x = av_mem, y = av_power, color = CoV_m)) + geom_point(position = "jitter") + geom_smooth(color = "#525252") + labs(x = "Average Memory (percentage)", y = "Average Power Draw (watts)", title = "Average Power Draw by Average Memory Usage for Hostname") # Power use generally increases as more gpu memory is used - you see a clear divide into two groups here.
```

```{R mem.util, echo = FALSE}
# Plot mem by util
Performance %>%
  ggplot(aes(x = av_mem, y = av_util, color = CoV_m)) + geom_point(position = "jitter") + geom_smooth(color = "#525252") + labs(x = "Average Memory (percentage)", y = "Average Utilisation (percentage)", title = "Average Utilisation by Average Memory Usage for Hostname") # Very clear trajectory of more memory corresponds to utilising  more gpu core
```
## Understanding (second iteration)

### Objectives and success criteria

This extended technical project explores "How could the computation of a terapixel visualisation be more efficient?"to identify useful insights about the stages of terapixel computation which are most resource intensive. An iniitail investigation into the three data sets provided by Newcastle University suggests that rending the visualisation is most time intensive, but that tiling and uploading the visualisation is most power intensive. GPU performance also appears to be loosely correlated with power consumption. However, CPU card usilisation of core and memory appear to be tightly correlated. A better understanding of what is driving the duration of the visualisation and the relationship of GPU cards performance is need to idenitfy useful insights. 

### Data mining goals and success criteria

To better understand the resource intensity of rendering a terapixel, the three data sets need to be explored as one whole picture... The second iteration of data mining aims to determine:

4. What is the variation in computation requirements for particular tiles? The data mining success criteria is identifying if there is a variation in computing requirements to determine which tiles are most intensive to produce. 

5. Are there particular GPU cards (based on their serial numbers) whose performance differs to other cards? The data mining success criteria is identifying the outlier GPU cards (for example, cards that are perpetually slow) to determine which cards could benefit from further investigation to improve computation performance.

## Data understanding (second iteration)

The details of data collection, description and quality outlined in the first iteration of data understanding also apply to this second iteration. 

### Data exploration

**Goal 4 (Tile computation)**

There is a significant amount of variation across the power consumption and render duration for each hostname. To better understand why this might be occurring, a heat map is created from the x and y co-ordinates  to identify if the intensity of the visualisation accounts for this. 

```{R heatmap, echo = FALSE}
# Create a heatmap co-ordinates from XY1 with colour as Total Render time
T1 <- AC7[, c("taskId", "duration")] # Create new tibble with taskId and duration time

TR2 <- left_join(T1, XY1, by = "taskId") # Join together T1 and XY1

TR2 %>%
  ggplot(aes(x = x, y = y, fill = duration)) + geom_tile() # Surfaces with intense texture take more time to render, for example stadium seats and roofs with different surfaces such as satelites. 
```

As mentioned in the background, the tile coordinates ultimately generate [this] (http://terapixel.wasabi.1024.s3.eu-central-1.wasabisys.com/vtour/index.html) visualisation of Newcastle upon Tyne. 

The heat map suggest that the more patterned edge of the stadium (perhaps seating), roofs with more texture (such as satellites) take longer to process. 

**Goal 5 (GPU serials)**

The slowest gpu cards are:

```{R s_gpu, }
# Filter out gpu cards that have the longest duration
Power_Duration3 <- Power_Duration2 %>%
  group_by(gpuSerial) %>%
  mutate(av_gpu_t = mean(duration))

S_gpu <- Power_Duration3 %>%
  filter(av_gpu_t >= 34) %>%
  arrange(desc(av_gpu_t)) %>%
  head(10)
```

Interestingly, the slowest gpu serials don't use much power. Why?

## Understanding (third iteration)

### Objectives and success criteria

This extended technical project explores "How could the computation of a terapixel visualisation be more efficient?"to identify useful insights about the stages of terapixel computation which are most resource intensive.... A better understanding of what tasking is needed....

### Data mining goals and success criteria

To better understand the resource intensity of rendering a terapixel, the three data sets need to be explored as one whole picture... The third iteration of data mining aims to determine:

6. What can we learn about the efficiency of the task scheduling process? **...**

## Data understanding (second iteration)

The details of data collection, description and quality outlined in the first iteration of data understanding also apply to this second iteration. 

### Data exploration

**Goal 6 (Tasking Efficiency)**
 
I think the tasking is trying to group based on performance requirement of the tile rendering, which is why we see the groups. Not sure how to prove. 

## Evaluation - How successful has it been? Provide evidence sing appropriate evaluation methodologies, and comment on the strengths/weaknesses of your evidence in answering this question.

### Assessment of results

- What are the future implications for work in this area? If applicable, which areas of extension work are now possible due to the foundational work you have performed in this project?

- execution time if often the best metric for accessing comuter performance (Hoefler)

- potential next steps. 

## Reflection
- A brief relfection on your personal and professional learning in undertaking this project. Here you can comment on how you found the process, what you learned about the technologies and methodologies you used, which aspects you found most difficult/straightforward, and any conclusions which will inform the way you undertake similar projects in future. 






\newpage

# Bibliography
